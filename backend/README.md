
# Changi Airport RAG Chatbot

This is a production-ready retrieval-augmented chatbot (RAG) for Changi Airport & Jewel, powered by:

- Google Gemini Pro (for embeddings and answer generation)
- Pinecone (vector database for semantic search)
- FastAPI (REST API)
- LangChain (prompt templating and document abstraction)

---

## ğŸ“ Project Structure
```
chatbot_app/backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api.py              # FastAPI endpoints
â”‚   â”œâ”€â”€ chatbot.py          # RAG pipeline logic
â”‚   â”œâ”€â”€ config.py           # Env config & constants
â”‚   â”œâ”€â”€ embeddings.py       # Gemini embedding setup
â”‚   â”œâ”€â”€ vector_store.py     # Pinecone operations
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ scraper.py      # Web scraping logic
â”‚       â””â”€â”€ cleaner.py      # HTML to clean text chunks
â”œâ”€â”€ scrapers/
â”‚   â””â”€â”€ data/
â”‚       â””â”€â”€ scraped_pages.json   # Raw scraped HTML pages
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ Dockerfile              # Docker setup
â”œâ”€â”€ main.py                 # FastAPI app entrypoint
â””â”€â”€ README.md               # Project documentation
```
---

## ğŸ”§ Setup Instructions

### 1. Clone and install dependencies

```bash
git clone https://your-company-repo-url/chatbot_app.git
cd chatbot_app
pip install -r requirements.txt
````

Ensure you are using Python 3.10+.

---

### 2. Set environment variables

Update `app/config.py` or load environment variables using `.env` file.

Required keys:

* GEMINI\_API\_KEY
* PINECONE\_API\_KEY
* PINECONE\_INDEX
* PINECONE\_ENV

---

### 3. Run the RAG data pipeline

This loads raw HTML â†’ cleans and chunks it â†’ embeds â†’ stores in Pinecone (without duplicating existing vectors).

```bash
python -m app.embed_store
```

---

### 4. Start the FastAPI server

```bash
uvicorn main:app --host 0.0.0.0 --port 8000
```

Docs available at:

[http://localhost:8000/docs](http://localhost:8000/docs)

---

## ğŸš€ API Endpoints

* **GET** `/ping`
  Returns: `"pong"`

* **POST** `/chat`
  Body:

  ```json
  {
    "query": "What is the Jewel Infinity Programme?"
  }
  ```

  Returns:
  Answer string generated by Gemini based on Pinecone context.

---

## ğŸ³ Docker Usage

### 1. Build the Docker image

```bash
docker build -t web-chatbot .
```

### 2. Run the container

```bash
docker run -p 8000:8000 web-chatbot
```

The FastAPI server will be available at `http://localhost:8000`.

---

## ğŸ§  System Behavior

* Only new document chunks are embedded and stored.
* Duplicate chunks are skipped using deterministic MD5 hashing.
* If no relevant info is found, the model replies with:
  `"Sorry, I could not find that in the documentation."`
* Embedding and retrieval are optimized with batching, retries, and parallelism.

---

## âœ… Example Workflow

1. Scrape content â†’ `scraped_pages.json`
2. Run `embed_store.py` to populate Pinecone
3. Start server
4. Call `/chat` with a user query
5. Gemini responds using real content only (via LangChain RAG prompt)

---

## ğŸ“Œ Notes

* Gemini model: `gemini-1.5-flash-latest`
* Embedding model: `models/text-embedding-004`
* Search type: `cosine similarity` via Pinecone
* No hallucinated responses â€” strictly grounded to context

